{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Twitter Auth Token\n",
    "\n",
    "twitter_auth_token = 'b7653d1625a75bd311de931ce4da8c8a73699479'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'NODE_MAJOR' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v18.18.0\n"
     ]
    }
   ],
   "source": [
    "# Install Node.js (because tweet-harvest built using Node.js)\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y ca-certificates curl gnupg\n",
    "!sudo mkdir -p /etc/apt/keyrings\n",
    "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
    "\n",
    "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
    "\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install nodejs -y\n",
    "\n",
    "!node -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "npm ERR! code ETARGET\n",
      "npm ERR! notarget No matching version found for tweet-harvest@2.2.8.\n",
      "npm ERR! notarget In most cases you or one of your dependencies are requesting\n",
      "npm ERR! notarget a package version that doesn't exist.\n",
      "\n",
      "npm ERR! A complete log of this run can be found in: C:\\Users\\Asus Vivobook\\AppData\\Local\\npm-cache\\_logs\\2024-04-27T13_23_46_663Z-debug-0.log\n"
     ]
    }
   ],
   "source": [
    "# Crawl Data\n",
    "\n",
    "filename = 'pajak.csv'\n",
    "search_keyword = 'pajak'\n",
    "limit = 1000\n",
    "\n",
    "!npx --yes tweet-harvest@2.2.8 -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token {twitter_auth_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Specify the path to your CSV file\n",
    "# file_path = f\"tweets-data/{filename}\"\n",
    "\n",
    "# # Read the CSV file into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "# # Display the DataFrame\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['full_text'] = df['full_text'].str.lower()\n",
    "\n",
    "\n",
    "# print('Case Folding Result : \\n')\n",
    "# print(df['full_text'].head(5))\n",
    "# print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string \n",
    "# import re \n",
    "\n",
    "\n",
    "# from nltk.tokenize import word_tokenize \n",
    "# from nltk.probability import FreqDist\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# def remove_tweet_special(text):\n",
    "#     text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "#     text = text.encode('ascii', 'replace').decode('ascii')\n",
    "#     text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "#     return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "# df['full_text'] = df['full_text'].apply(remove_tweet_special)\n",
    "\n",
    "\n",
    "# def remove_number(text):\n",
    "#     return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(remove_number)\n",
    "\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# def remove_whitespace_LT(text):\n",
    "#     return text.strip()\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(remove_whitespace_LT)\n",
    "\n",
    "\n",
    "# def remove_whitespace_multiple(text):\n",
    "#     return re.sub('\\s+',' ',text)\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(remove_whitespace_multiple)\n",
    "\n",
    "\n",
    "# def remove_singl_char(text):\n",
    "#     return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(remove_singl_char)\n",
    "\n",
    "\n",
    "# def word_tokenize_wrapper(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# df['tweet_tokens'] = df['full_text'].apply(word_tokenize_wrapper)\n",
    "\n",
    "# print('Tokenizing Result : \\n') \n",
    "# print(df['tweet_tokens'].head())\n",
    "# print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# # get stopword indonesia\n",
    "# list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# # ---------------------------- manualy add stopword  ------------------------------------\n",
    "# # append additional stopword\n",
    "# list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "#                        'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "#                        'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "#                        'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "#                        'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "#                        'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "#                        '&amp', 'yah'])\n",
    "\n",
    "# # ----------------------- add stopword from txt file ------------------------------------\n",
    "# # read txt stopword using pandas\n",
    "# txt_stopword = pd.read_csv(\"stopwords.txt\", names=[\"stopwords\"], header = None)\n",
    "\n",
    "# # convert stopword string to list & append additional stopword\n",
    "# list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# # ---------------------------------------------------------------------------------------\n",
    "\n",
    "# # convert list to dictionary\n",
    "# list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "# #remove stopword pada list token\n",
    "# def stopwords_removal(words):\n",
    "#     return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "# df['tweet_tokens_WSW'] = df['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "# print(df['tweet_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sastrawi\n",
    "# %pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import Sastrawi package\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# import swifter\n",
    "\n",
    "\n",
    "# # create stemmer\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# # stemmed\n",
    "# def stemmed_wrapper(term):\n",
    "#     return stemmer.stem(term)\n",
    "\n",
    "# term_dict = {}\n",
    "\n",
    "# for document in df['tweet_tokens_WSW']:\n",
    "#     for term in document:\n",
    "#         if term not in term_dict:\n",
    "#             term_dict[term] = ' '\n",
    "            \n",
    "# print(len(term_dict))\n",
    "# print(\"------------------------\")\n",
    "\n",
    "# for term in term_dict:\n",
    "#     term_dict[term] = stemmed_wrapper(term)\n",
    "#     print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "# print(term_dict)\n",
    "# print(\"------------------------\")\n",
    "\n",
    "\n",
    "# # apply stemmed term to dataframe\n",
    "# def get_stemmed_term(document):\n",
    "#     return [term_dict[term] for term in document]\n",
    "\n",
    "# df['tweet_tokens_stemmed'] = df['tweet_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "# print(df['tweet_tokens_stemmed'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
