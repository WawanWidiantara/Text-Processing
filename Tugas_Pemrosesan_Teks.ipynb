{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5n2fVxcP0Ne",
        "outputId": "3e7f7fc5-4ca5-4239-87aa-3ba37a1211eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Asus\n",
            "[nltk_data]     Vivobook\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Asus\n",
            "[nltk_data]     Vivobook\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_document(document):\n",
        "    tokens = word_tokenize(document.lower())\n",
        "    stop_words = set(stopwords.words('indonesian'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "doc1 = \"elearning di PTIIK diatas jam 6 malam kok selalu gak bisa dibuka ya?\"\n",
        "doc2 = \"ub tidak punya lahan parkir yang layak. Dan jalanan terlalu ramai karena di buka untuk umum. Seperti jalan tol saja. Brawijaya oh brawijaya\"\n",
        "doc3 = \"Kelas Arsitektur dan Organisasi Komputer penuh, apakah tidak dibuka kelas lagi. Rugi kalo saya bisa ngambil 24 SKS tapi baru 18 SKS yg terpenuhi\"\n",
        "doc4 = \"Informasi tata cara daftar ulang bagi mahasiswa baru PTIIK kurang jelas. Sehingga ketika tanggal terakhir syarat penyerahan berkas daftar ulang, banyak mahasiswa baru yang tidak membawa salah satu syarat daftar ulangnya\"\n",
        "\n",
        "preprocessed_doc1 = preprocess_document(doc1)\n",
        "preprocessed_doc2 = preprocess_document(doc2)\n",
        "preprocessed_doc3 = preprocess_document(doc3)\n",
        "preprocessed_doc4 = preprocess_document(doc4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3OWx2qXQBfC",
        "outputId": "fe2ab0da-b911-4a89-d047-92bf8eaf9ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 1:\n",
            "['elearn', 'ptiik', 'diata', 'jam', '6', 'malam', 'gak', 'dibuka', 'ya']\n",
            "\n",
            "Hasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 2:\n",
            "['ub', 'lahan', 'parkir', 'layak', 'jalanan', 'ramai', 'buka', 'jalan', 'tol', 'brawijaya', 'oh', 'brawijaya']\n",
            "\n",
            "Hasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 3:\n",
            "['kela', 'arsitektur', 'organisasi', 'komput', 'penuh', 'dibuka', 'kela', 'rugi', 'kalo', 'ngambil', '24', 'sk', '18', 'sk', 'yg', 'terpenuhi']\n",
            "\n",
            "Hasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 4:\n",
            "['informasi', 'tata', 'daftar', 'ulang', 'mahasiswa', 'ptiik', 'tanggal', 'syarat', 'penyerahan', 'berka', 'daftar', 'ulang', 'mahasiswa', 'membawa', 'salah', 'syarat', 'daftar', 'ulangnya']\n"
          ]
        }
      ],
      "source": [
        "print(\"Hasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 1:\")\n",
        "print(preprocessed_doc1)\n",
        "print(\"\\nHasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 2:\")\n",
        "print(preprocessed_doc2)\n",
        "print(\"\\nHasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 3:\")\n",
        "print(preprocessed_doc3)\n",
        "print(\"\\nHasil Tokenisasi, Filtering, dan Stemming untuk Dokumen 4:\")\n",
        "print(preprocessed_doc4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
